---
name: 📝 리뷰 과제
about: YB심화 세션 회고 템플릿
title: '[Week 1] 1주차 리뷰 - 정유진'
labels: ['review','Homework']
assignees: ''
---

## 주제
<!-- 이번 주차에 다룬 주요 주제를 작성해주세요 -->
---
## YB심화 세션 운영방식 결정
- 사전 과제는 교재 읽고 자신이 나누고 싶은 이야기 정리해오는 것
- 복습과제, 논의한 내용 중 어려웠던 것이나 혼동되는 것, 함께 논의해서 정답을 찾지 못한 것들을 정리해오기
- 발제는 머신러닝 인터뷰 가이드라는 책으로 진행

## 2장 ML 기본 지식 (인사이드 머신러닝 인터뷰)
- 경사하강법이 쓰이는 종류와 차이점
- Nan 데이터 처리한 경험
- Overfitting에 대한 본인의 경험
- XAI 해석 가능한 인공지능
- 딥러닝과 머신러닝, 지도와 비지도 

## 내용
---
<!-- 주요 개념과 내용을 정리해주세요 -->
### 핵심 개념 1 | 경사하강법
---
#### 경사하강법은 어디에 사용되는가?
- 경사하강법(Gradient Descent)은 머신러닝 **모델에서 모델의 파라미터를 최적화 문제 해결을 위한 핵심 알고리즘**
- 모델의 예측 값과 실제 값 간의 차이를 나타내는 손실 함수(loss function)의 기울기(gradient)를 이용하여 파라미터를 업데이트
- 파라미터를 얼마나 업데이트할 것인지를 결정하는 것이 learning rate(학습률, step size) (스칼라)
- 함수의 기울기 정보를 이용해 파라미터를 반복적으로 조정하며 손실 함수의 최솟값을 찾는 기법
![alt text](image.png)

- **산 정상에서 안개 속에서 하산하는 과정**에 비유할 수 있는데, 눈앞의 **지형 경사만을 참고해 가장 가파른 방향으로 단계적으로 이동하는 원리**임.
$\theta_{t+1} = \theta_t - \eta \cdot \nabla J(\theta_t)$

- 현재 위치에서 주변 경사 측정  
- 경사 방향 반대(하강 시)로 발걸음 이동  
- 이동 거리 = (학습률 η) × (경사도)  
- 극소점 도달 시 반복 종료  

```python
θ_{t+1} = θ_t - η·∇J(θ_t)  # θ: 파라미터, η: 학습률, ∇J: 기울기
```
경사도가 클수록 큰 보폭으로 이동하며, 극소점 근처에선 작은 보폭으로 정밀 조정

#### 손실 함수 최소화
** 손실 함수 $$ J(θ) $$의 최소값 탐색 | **
$$ θ^* = \argmin_θ J(θ) $$
** 경사도 계산 | **
$$
∇J(θ) = \left(\frac{\partial J}{\partial θ_1}, \frac{\partial J}{\partial θ_2}, ..., \frac{\partial J}{\partial θ_n}\right)
$$
파라미터 업데이트 규칙 |
$$
θ_{new} = θ_{old} - η·∇J(θ_{old})
$$



### 4. 경사하강법 유형 비교

각 경사하강법의 특징과 수식을 비교한 표입니다:

| 최적화 기법 | 적용 상황 | 수식 | 설명 |
|------------|--------------------|----------------|----------------|
| Mini-batch GD | 데이터가 많고 연산 속도가 중요할 때 | $$ θ_{t+1} = θ_t - η\frac{1}{n}\sum_{i=1}^n\nabla J_i(θ_t) $$ | n개의 미니배치 샘플에 대해 평균 기울기 계산. η는 학습률 |
| Batch GD | 데이터가 적고 정확한 해석이 필요할 때 | $$ θ_{t+1} = θ_t - η\nabla J(θ_t) $$ | 전체 데이터셋에 대한 기울기 계산 |
| SGD + Momentum | 빠른 학습 속도가 필요할 때 | $$ v_t = γv_{t-1} + η\nabla J(θ_t) \\ θ_{t+1} = θ_t - v_t $$ | γ는 모멘텀 계수, 이전 기울기 정보 활용 |
| Adagrad/RMSprop | 희소한 데이터나 자연어 처리 시 | $$ θ_{t+1} = θ_t - \frac{η}{\sqrt{G_t + ε}}\nabla J(θ_t) $$ | G_t는 과거 기울기 제곱의 합, ε은 분모 0 방지 상수 |
| Adam | 일반적인 딥러닝 학습 | $$ m_t = β_1m_{t-1} + (1-β_1)\nabla J(θ_t) \\ v_t = β_2v_{t-1} + (1-β_2)(\nabla J(θ_t))^2 \\ θ_{t+1} = θ_t - η\frac{m_t}{\sqrt{v_t} + ε} $$ | 모멘텀과 RMSprop 결합, β₁,β₂는 이동평균 계수 |

Citations:
[1] https://pplx-res.cloudinary.com/image/upload/v1738570434/user_uploads/OgFMyBHDuGQwJDI/image.jpg

---
Perplexity로부터의 답변: pplx.ai/share


### 5. 발전된 최적화 기법
**Adam Optimizer**:  
- 모멘텀(과속 방지) + RMSprop(학습률 자동 조정) 통합  
- 업데이트 식:  
$$
m_t = β_1·m_{t-1} + (1-β_1)·g_t \\
v_t = β_2·v_{t-1} + (1-β_2)·g_t^2 \\
θ_t = θ_{t-1} - η·\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + ε}
$$
여기서 $$ \hat{m}_t $$, $$ \hat{v}_t $$는 편향 보정 항

---
Perplexity로부터의 답변: pplx.ai/share

#### 경사하강법에서 learning rate 설정하기
- Learning rate가 너무 크면 모델이 최적점을 지나칠 수 있고(overshooting), 너무 작으면 학습 속도가 느려질 수 있고, local minimum에 빠질 위험이 있음.
- Learning rate을 0.1, 0.01, 0.001로 떨어 뜨리면서 실험하는 것이 좋다. 큰값 -> 작은 값

**스케줄링**
- 특정 고정값이 아니라 규칙에 따라 학습률 적용하는 방식 
- 특정한 epoch에 따라 감소하는 Step Decay, 코사인함수를 따라 연속적으로 감소하는 Cosine Decay, 단순 감소하는 Linear Dacay, 제곱근의 역수로 감소하는 Inverse Sqrt Decay 등 
- CLR(Cyclical learning rates) : 최대 학습률(max_lr)과 최저 학습률(lr) 사이 값을 순환하게 하는 방법 &  step size는 최저 지점과 최고 지점 사이 길이

#### 경사하강법의 종류와 쓰임새
#### Adam Optimaizer가 많이 쓰이는 이유 (개념과 수식을 중심으로)
- 
- 
### 핵심 개념 2 | 머신러닝과 딥러닝
---
#### 머신러닝과 딥러닝, 지도 비지도 분류 명확히 하기
#### 블랙박스/화이트박스
#### XAI, SHAP 등 해석 가능한 인공지능 
#### ADD | ML면접에서 헷갈리는 지도 비지도

### 핵심 개념 3 | 데이터 전처리/모델의 과대 과소적합
---
#### 오버피팅이란?
#### 오버피팅을 판별하는 일반적인 기준은? (그래프)
#### 데이터에서 Nan값은 항상 제거되야하는가?
#### 다양한 보간법

## 참고 문헌
---
<!-- 참고한 자료의 제목과 링크를 작성해주세요 -->
1. [위키백과 경사하강법](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95)
2. [모두의 연구소 글](https://modulabs.co.kr/blog/importance-of-learning-rate-and-finding-appropriate-learning-rate)
