---
name: 📝 리뷰 과제
about: YB심화 세션 회고 템플릿
title: '[Week 1] 1주차 리뷰 - 정유진'
labels: ['review','Homework']
assignees: ''
---

## 주제
<!-- 이번 주차에 다룬 주요 주제를 작성해주세요 -->
---
## YB심화 세션 운영방식 결정
- 사전 과제는 교재 읽고 자신이 나누고 싶은 이야기 정리해오는 것
- 복습과제, 논의한 내용 중 어려웠던 것이나 혼동되는 것, 함께 논의해서 정답을 찾지 못한 것들을 정리해오기
- 발제는 머신러닝 인터뷰 가이드라는 책으로 진행

## 2장 ML 기본 지식 (인사이드 머신러닝 인터뷰)
- 경사하강법이 쓰이는 종류와 차이점
- Nan 데이터 처리한 경험
- Overfitting에 대한 본인의 경험
- XAI 해석 가능한 인공지능
- 딥러닝과 머신러닝, 지도와 비지도 

## 내용
---
<!-- 주요 개념과 내용을 정리해주세요 -->
### 핵심 개념 1 | 경사하강법
---
#### 경사하강법은 어디에 사용되는가?
- 경사하강법(Gradient Descent)은 머신러닝 **모델에서 모델의 파라미터를 최적화 문제 해결을 위한 핵심 알고리즘**
- 모델의 예측 값과 실제 값 간의 차이를 나타내는 손실 함수(loss function)의 기울기(gradient)를 이용하여 파라미터를 업데이트
- 파라미터를 얼마나 업데이트할 것인지를 결정하는 것이 learning rate(학습률, step size) (스칼라)
- 함수의 기울기 정보를 이용해 파라미터를 반복적으로 조정하며 손실 함수의 최솟값을 찾는 기법
![alt text](image.png)

- **산 정상에서 안개 속에서 하산하는 과정**에 비유할 수 있는데, 눈앞의 **지형 경사만을 참고해 가장 가파른 방향으로 단계적으로 이동하는 원리**임.
$\theta_{t+1} = \theta_t - \eta \cdot \nabla J(\theta_t)$

#### 경사하강법에서 learning rate 설정하기
- Learning rate가 너무 크면 모델이 최적점을 지나칠 수 있고(overshooting), 너무 작으면 학습 속도가 느려질 수 있고, local minimum에 빠질 위험이 있음.
- Learning rate을 0.1, 0.01, 0.001로 떨어 뜨리면서 실험하는 것이 좋다. 큰값 -> 작은 값

**스케줄링**
- 특정 고정값이 아니라 규칙에 따라 학습률 적용하는 방식 
- 특정한 epoch에 따라 감소하는 Step Decay, 코사인함수를 따라 연속적으로 감소하는 Cosine Decay, 단순 감소하는 Linear Dacay, 제곱근의 역수로 감소하는 Inverse Sqrt Decay 등 
- CLR(Cyclical learning rates) : 최대 학습률(max_lr)과 최저 학습률(lr) 사이 값을 순환하게 하는 방법 &  step size는 최저 지점과 최고 지점 사이 길이

#### 경사하강법의 종류와 쓰임새
#### Adam Optimaizer가 많이 쓰이는 이유 (개념과 수식을 중심으로)
- 
- 
### 핵심 개념 2 | 머신러닝과 딥러닝
---
#### 머신러닝과 딥러닝, 지도 비지도 분류 명확히 하기
#### 블랙박스/화이트박스
#### XAI, SHAP 등 해석 가능한 인공지능 
#### ADD | ML면접에서 헷갈리는 지도 비지도

### 핵심 개념 3 | 데이터 전처리/모델의 과대 과소적합
---
#### 오버피팅이란?
#### 오버피팅을 판별하는 일반적인 기준은? (그래프)
#### 데이터에서 Nan값은 항상 제거되야하는가?
#### 다양한 보간법

## 참고 문헌
---
<!-- 참고한 자료의 제목과 링크를 작성해주세요 -->
1. [위키백과 경사하강법](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95)
2. [모두의 연구소 글](https://modulabs.co.kr/blog/importance-of-learning-rate-and-finding-appropriate-learning-rate)
